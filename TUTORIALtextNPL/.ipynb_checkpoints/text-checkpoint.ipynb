{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c62384ae-e3c3-41ec-a05e-9e4ff7505364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_algebra import dot, Vector\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2a018f9-54e8-4fe3-9361-36883916d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1: Vector, v2: Vector) -> float:\n",
    "    return dot(v1, v2) / math.sqrt(dot(v1, v1) * dot(v2, v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a2d993-bde2-4339-b87b-6b5b5bfc41b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"red\", \"green\", \"blue\", \"yellow\", \"black\", \"\"]\n",
    "nouns = [\"bed\", \"car\", \"boat\", \"cat\"]\n",
    "verbs = [\"is\", \"was\", \"seems\"]\n",
    "adverbs = [\"very\", \"quite\", \"extremely\", \"\"]\n",
    "adjectives = [\"slow\", \"fast\", \"soft\", \"hard\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bd6e4f4-dee6-4b48-a5fb-ede5e576ce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentence() -> str:\n",
    "    return \" \".join([\n",
    "    \"The\",\n",
    "    random.choice(colors),\n",
    "    random.choice(nouns),\n",
    "    random.choice(verbs),\n",
    "    random.choice(adverbs),\n",
    "    random.choice(adjectives),\n",
    "    \".\"\n",
    "    ])\n",
    "NUM_SENTENCES = 50\n",
    "random.seed(0)\n",
    "sentences = [make_sentence() for _ in range(NUM_SENTENCES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b309266e-337e-4fc2-8b05-68d27a4af58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "Tensor = list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61dda9b1-3f55-4d69-920e-fbe0ebb153eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, words: List[str] = None) -> None:\n",
    "        self.w2i: Dict[str, int] = {} # mapping word -> word_id\n",
    "        self.i2w: Dict[int, str] = {} # mapping word_id -> word\n",
    "        for word in (words or []): # If words were provided,\n",
    "            self.add(word) # add them.\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        \"\"\"how many words are in the vocabulary\"\"\"\n",
    "        return len(self.w2i)\n",
    "    \n",
    "    def add(self, word: str) -> None:\n",
    "        if word not in self.w2i: # If the word is new to us:\n",
    "            word_id = len(self.w2i) # Find the next id.\n",
    "            self.w2i[word] = word_id # Add to the word -> word_id map.\n",
    "            self.i2w[word_id] = word # Add to the word_id -> word map.\n",
    "    \n",
    "    def get_id(self, word: str) -> int:\n",
    "        \"\"\"return the id of the word (or None)\"\"\"\n",
    "        return self.w2i.get(word)\n",
    "    \n",
    "    def get_word(self, word_id: int) -> str:\n",
    "        \"\"\"return the word with the given id (or None)\"\"\"\n",
    "        return self.i2w.get(word_id)\n",
    "    \n",
    "    def one_hot_encode(self, word: str) -> Tensor:\n",
    "        word_id = self.get_id(word)\n",
    "        assert word_id is not None, f\"unknown word {word}\"\n",
    "        return [1.0 if i == word_id else 0.0 for i in range(self.size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c437c029-6004-41fd-8f01-de283f63796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary([\"a\", \"b\", \"c\"])\n",
    "assert vocab.size == 3, \"there are 3 words in the vocab\"\n",
    "assert vocab.get_id(\"b\") == 1, \"b should have word_id 1\"\n",
    "assert vocab.one_hot_encode(\"b\") == [0, 1, 0]\n",
    "assert vocab.get_id(\"z\") is None, \"z is not in the vocab\"\n",
    "assert vocab.get_word(2) == \"c\", \"word_id 2 should be c\"\n",
    "vocab.add(\"z\")\n",
    "assert vocab.size == 4, \"now there are 4 words in the vocab\"\n",
    "assert vocab.get_id(\"z\") == 3, \"now z should have id 3\"\n",
    "assert vocab.one_hot_encode(\"z\") == [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0145b328-239e-46c4-826b-68228f25798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_vocab(vocab: Vocabulary, filename: str) -> None:\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(vocab.w2i, f) # Only need to save w2i\n",
    "def load_vocab(filename: str) -> Vocabulary:\n",
    "    vocab = Vocabulary()\n",
    "    with open(filename) as f:\n",
    "        # Load w2i and generate i2w from it\n",
    "        vocab.w2i = json.load(f)\n",
    "        vocab.i2w = {id: word for word, id in vocab.w2i.items()}\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fe000fc-5497-459a-a29d-8f0a2b5150b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Iterable, Tuple\n",
    "from scratch.deep_learning import Layer, Tensor, random_tensor, zeros_like\n",
    "class Embedding(Layer):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int) -> None:\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # One vector of size embedding_dim for each desired embedding\n",
    "        self.embeddings = random_tensor(num_embeddings, embedding_dim)\n",
    "        self.grad = zeros_like(self.embeddings)\n",
    "        # Save last input id\n",
    "        self.last_input_id = None\n",
    "    def forward(self, input_id: int) -> Tensor:\n",
    "        \"\"\"Just select the embedding vector corresponding to the input id\"\"\"\n",
    "        self.input_id = input_id # remember for use in backpropagation\n",
    "        return self.embeddings[input_id]\n",
    "    def backward(self, gradient: Tensor) -> None:\n",
    "        # Zero out the gradient corresponding to the last input.\n",
    "        # This is way cheaper than creating a new all-zero tensor each time.\n",
    "        if self.last_input_id is not None:\n",
    "            zero_row = [0 for _ in range(self.embedding_dim)]\n",
    "            self.grad[self.last_input_id] = zero_row\n",
    "        self.last_input_id = self.input_id\n",
    "        self.grad[self.input_id] = gradient\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        return [self.embeddings]\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        return [self.grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "417aa883-3254-4c6e-a913-48b3f59aa815",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedding(Embedding):\n",
    "    def __init__(self, vocab: Vocabulary, embedding_dim: int) -> None:\n",
    "        # Call the superclass constructor\n",
    "        super().__init__(vocab.size, embedding_dim)\n",
    "        # And hang onto the vocab\n",
    "        self.vocab = vocab\n",
    "    def __getitem__(self, word: str) -> Tensor:\n",
    "        word_id = self.vocab.get_id(word)\n",
    "        if word_id is not None:\n",
    "            return self.embeddings[word_id]\n",
    "        else:\n",
    "            return None\n",
    "    def closest(self, word: str, n: int = 5) -> List[Tuple[float, str]]:\n",
    "        \"\"\"Returns the n closest words based on cosine similarity\"\"\"\n",
    "        vector = self[word]\n",
    "        # Compute pairs (similarity, other_word), and sort most similar first\n",
    "        scores = [(cosine_similarity(vector, self.embeddings[i]), other_word) for other_word, i in self.vocab.w2i.items()]\n",
    "        scores.sort(reverse=True)\n",
    "        return scores[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d805995-78c0-4060-8a2a-e34bea67ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# This is not a great regex, but it works on our data.\n",
    "tokenized_sentences = [re.findall(\"[a-z]+|[.]\", sentence.lower()) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18e50464-af13-4b09-8a82-3162ea8b97c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary (that is, a mapping word -> word_id) based on our text.\n",
    "vocab = Vocabulary(word for sentence_words in tokenized_sentences for word in sentence_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3055a162-235e-48bd-ab3a-039e4d56e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.deep_learning import Tensor, one_hot_encode\n",
    "inputs: List[int] = []\n",
    "targets: List[Tensor] = []\n",
    "for sentence in tokenized_sentences:\n",
    "    for i, word in enumerate(sentence): # For each word\n",
    "        for j in [i - 2, i - 1, i + 1, i + 2]: # take the nearby locations\n",
    "            if 0 <= j < len(sentence): # that aren't out of bounds\n",
    "                nearby_word = sentence[j] # and get those words.\n",
    "                # Add an input that's the original word_id\n",
    "                inputs.append(vocab.get_id(word))\n",
    "                # Add a target that's the one-hot-encoded nearby word\n",
    "                targets.append(vocab.one_hot_encode(nearby_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e35e288-c8da-44dd-baa2-d2449297408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.deep_learning import Sequential, Linear\n",
    "random.seed(0)\n",
    "EMBEDDING_DIM = 5 # seems like a good size\n",
    "# Define the embedding layer separately, so we can reference it.\n",
    "embedding = TextEmbedding(vocab=vocab, embedding_dim=EMBEDDING_DIM)\n",
    "model = Sequential([\n",
    "# Given a word (as a vector of word_ids), look up its embedding.\n",
    "embedding,\n",
    "# And use a linear layer to compute scores for \"nearby words.\"\n",
    "Linear(input_dim=EMBEDDING_DIM, output_dim=vocab.size)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42161446-51db-4678-b91b-07a992fc30ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2403.2259775721227\n",
      "[(1.0, 'black'), (0.9821293900663808, 'blue'), (0.8709372536245938, 'yellow'), (0.8560680073921021, 'green'), (0.7209977273875182, 'red')]\n",
      "[(1.0, 'slow'), (0.8900872652719086, 'fast'), (0.8846955683864672, 'hard'), (0.8505709819185355, 'soft'), (0.3979688002551069, 'quite')]\n",
      "[(1.0, 'car'), (0.8545578601895432, 'cat'), (0.7977433731597623, 'bed'), (0.6982593591613964, 'boat'), (0.22838470790337795, 'black')]\n"
     ]
    }
   ],
   "source": [
    "from scratch.deep_learning import SoftmaxCrossEntropy, Momentum, GradientDescent\n",
    "loss = SoftmaxCrossEntropy()\n",
    "optimizer = GradientDescent(learning_rate=0.01)\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0.0\n",
    "    for input, target in zip(inputs, targets):\n",
    "        predicted = model.forward(input)\n",
    "        epoch_loss += loss.loss(predicted, target)\n",
    "        gradient = loss.gradient(predicted, target)\n",
    "        model.backward(gradient)\n",
    "        optimizer.step(model)\n",
    "print(epoch, epoch_loss) # Print the loss\n",
    "print(embedding.closest(\"black\")) # and also a few nearest words\n",
    "print(embedding.closest(\"slow\")) # so we can see what's being\n",
    "print(embedding.closest(\"car\")) # learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c720522d-7669-4bba-94ed-971ddb4e94a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1004"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
